"""Production deployment utilities for QISA models.

This module provides comprehensive production deployment capabilities including:
- Model serving with REST/gRPC APIs
- Auto-scaling based on load and performance
- Model versioning and A/B testing
- Health monitoring and alerting
- Multi-region deployment support
- Performance optimization for inference
- Security and authentication
"""

import os
import time
import json
import logging
import asyncio
import threading
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import uuid
from datetime import datetime

try:
    import torch
    import torch.nn as nn
    import numpy as np
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    np = None

# Optional production dependencies
try:
    import aiohttp
    from aiohttp import web
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class QISADeploymentConfig:
    """Configuration for QISA production deployment."""
    # Model serving
    model_name: str = "qisa_model"
    model_version: str = "1.0.0"
    serving_port: int = 8080
    max_batch_size: int = 32
    max_sequence_length: int = 2048
    timeout_seconds: float = 30.0
    
    # Performance
    enable_model_compilation: bool = True
    enable_mixed_precision: bool = True
    enable_tensorrt: bool = False
    enable_onnx_export: bool = True
    
    # Scaling
    enable_auto_scaling: bool = True
    min_replicas: int = 1
    max_replicas: int = 10
    target_cpu_utilization: float = 70.0
    target_memory_utilization: float = 80.0
    scale_up_threshold: float = 85.0
    scale_down_threshold: float = 30.0
    
    # Monitoring
    enable_health_checks: bool = True
    health_check_interval: int = 30
    enable_metrics_collection: bool = True
    metrics_port: int = 9090
    
    # Security
    enable_authentication: bool = True
    api_key_required: bool = True
    rate_limiting: bool = True
    requests_per_minute: int = 1000
    
    # Deployment
    deployment_strategy: str = "rolling"  # "blue_green", "canary", "rolling"
    canary_percentage: float = 10.0
    rollback_threshold_error_rate: float = 5.0
    
    # Multi-region
    enable_multi_region: bool = False
    regions: List[str] = field(default_factory=lambda: ["us-east-1"])
    load_balancing_strategy: str = "round_robin"  # "least_connections", "geographic"


class ModelVersionManager:
    """Manages model versions and deployments."""
    
    def __init__(self, config: QISADeploymentConfig):
        self.config = config
        self.models = {}  # version -> model
        self.active_version = None
        self.canary_version = None
        self.canary_traffic_percentage = 0.0
        
        self.deployment_history = []
        self.rollback_history = []
    
    def register_model(self, version: str, model: nn.Module, metadata: Dict[str, Any] = None):
        """Register a new model version."""
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorch not available")
        
        # Optimize model for inference
        optimized_model = self._optimize_for_inference(model)
        
        self.models[version] = {
            "model": optimized_model,
            "metadata": metadata or {},
            "registration_time": datetime.now(),
            "inference_count": 0,
            "total_latency": 0.0,
            "error_count": 0
        }
        
        logger.info(f"Registered model version: {version}")
        
        # Set as active if first model
        if self.active_version is None:
            self.set_active_version(version)
    
    def _optimize_for_inference(self, model: nn.Module) -> nn.Module:
        """Optimize model for production inference."""
        model.eval()
        
        # Apply optimizations
        if self.config.enable_model_compilation and hasattr(torch, 'compile'):
            try:
                model = torch.compile(model, mode="max-autotune")
                logger.info("Applied torch.compile optimization")
            except Exception as e:
                logger.warning(f"Failed to compile model: {e}")
        
        if self.config.enable_mixed_precision:
            try:
                model = model.half()
                logger.info("Applied mixed precision optimization")
            except Exception as e:
                logger.warning(f"Failed to apply mixed precision: {e}")
        
        # Optimize for inference (if QISA model)
        if hasattr(model, 'optimize_for_inference'):
            try:
                model.optimize_for_inference()
                logger.info("Applied QISA inference optimizations")
            except Exception as e:
                logger.warning(f"Failed to optimize QISA model: {e}")
        
        return model
    
    def set_active_version(self, version: str):
        """Set the active model version."""
        if version not in self.models:
            raise ValueError(f"Model version {version} not found")
        
        old_version = self.active_version
        self.active_version = version
        
        self.deployment_history.append({
            "timestamp": datetime.now(),
            "action": "activate",
            "version": version,
            "previous_version": old_version
        })
        
        logger.info(f"Activated model version: {version}")
    
    def start_canary_deployment(self, version: str, traffic_percentage: float = 10.0):
        """Start canary deployment for a new version."""
        if version not in self.models:
            raise ValueError(f"Model version {version} not found")
        
        if traffic_percentage <= 0 or traffic_percentage >= 100:
            raise ValueError("Canary traffic percentage must be between 0 and 100")
        
        self.canary_version = version
        self.canary_traffic_percentage = traffic_percentage
        
        self.deployment_history.append({
            "timestamp": datetime.now(),
            "action": "canary_start",
            "version": version,
            "traffic_percentage": traffic_percentage
        })
        
        logger.info(f"Started canary deployment: {version} ({traffic_percentage}% traffic)")
    
    def promote_canary(self):
        """Promote canary version to active."""
        if self.canary_version is None:
            raise ValueError("No canary deployment in progress")
        
        old_active = self.active_version
        self.set_active_version(self.canary_version)
        
        self.canary_version = None
        self.canary_traffic_percentage = 0.0
        
        logger.info(f"Promoted canary {self.active_version} to active (replaced {old_active})")
    
    def rollback_deployment(self, reason: str = "Manual rollback"):
        """Rollback to previous version."""
        if len(self.deployment_history) < 2:
            raise ValueError("No previous version to rollback to")
        
        # Find previous active version
        previous_active = None
        for deployment in reversed(self.deployment_history[:-1]):
            if deployment["action"] == "activate":
                previous_active = deployment["version"]
                break
        
        if previous_active is None:
            raise ValueError("Cannot determine previous version")
        
        old_version = self.active_version
        self.set_active_version(previous_active)
        
        self.rollback_history.append({
            "timestamp": datetime.now(),
            "from_version": old_version,
            "to_version": previous_active,
            "reason": reason
        })
        
        logger.warning(f"Rolled back from {old_version} to {previous_active}: {reason}")
    
    def get_model_for_inference(self, request_id: str = None) -> Tuple[nn.Module, str]:
        """Get model for inference (handles canary routing)."""
        # Determine which model to use
        if (self.canary_version is not None and 
            np.random.random() < (self.canary_traffic_percentage / 100.0)):
            version = self.canary_version
        else:
            version = self.active_version
        
        if version is None or version not in self.models:
            raise ValueError(f"No active model available")
        
        return self.models[version]["model"], version
    
    def record_inference_metrics(self, version: str, latency: float, success: bool):
        """Record inference metrics for a model version."""
        if version in self.models:
            model_info = self.models[version]
            model_info["inference_count"] += 1
            model_info["total_latency"] += latency
            if not success:
                model_info["error_count"] += 1
    
    def get_model_metrics(self) -> Dict[str, Any]:
        """Get metrics for all model versions."""
        metrics = {}
        
        for version, model_info in self.models.items():
            count = model_info["inference_count"]
            total_latency = model_info["total_latency"]
            error_count = model_info["error_count"]
            
            metrics[version] = {
                "inference_count": count,
                "average_latency_ms": (total_latency / count * 1000) if count > 0 else 0,
                "error_rate_percent": (error_count / count * 100) if count > 0 else 0,
                "success_rate_percent": ((count - error_count) / count * 100) if count > 0 else 0
            }
        
        return metrics


class QISAInferenceServer:
    """High-performance inference server for QISA models."""
    
    def __init__(self, config: QISADeploymentConfig):
        self.config = config
        self.version_manager = ModelVersionManager(config)
        self.health_monitor = HealthMonitor(config)
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.batch_processor = BatchProcessor(config)
        
        # Request tracking
        self.active_requests = {}
        self.request_history = []
        
        # Performance metrics
        self.total_requests = 0
        self.total_errors = 0
        self.total_latency = 0.0
        
        # Security
        self.api_keys = set()
        self.rate_limiter = RateLimiter(config.requests_per_minute)
    
    async def initialize(self):
        """Initialize the inference server."""
        logger.info("Initializing QISA inference server...")
        
        # Start background tasks
        asyncio.create_task(self.batch_processing_loop())
        asyncio.create_task(self.health_monitoring_loop())
        asyncio.create_task(self.metrics_collection_loop())
        
        logger.info("QISA inference server initialized")
    
    def add_api_key(self, api_key: str):
        """Add valid API key."""
        self.api_keys.add(api_key)
    
    def validate_request(self, request: Dict[str, Any], api_key: str = None) -> Tuple[bool, str]:
        """Validate incoming request."""
        # API key validation
        if self.config.api_key_required:
            if not api_key or api_key not in self.api_keys:
                return False, "Invalid API key"
        
        # Rate limiting
        if self.config.rate_limiting:
            if not self.rate_limiter.allow_request():
                return False, "Rate limit exceeded"
        
        # Request validation
        if "input" not in request:
            return False, "Missing 'input' field"
        
        if not isinstance(request["input"], (list, dict)):
            return False, "Invalid input format"
        
        return True, "Valid"
    
    async def handle_inference_request(self, request: Dict[str, Any], 
                                     api_key: str = None) -> Dict[str, Any]:
        """Handle single inference request."""
        request_id = str(uuid.uuid4())
        start_time = time.time()
        
        # Validate request
        valid, message = self.validate_request(request, api_key)
        if not valid:
            return {
                "request_id": request_id,
                "error": message,
                "status": "error"
            }
        
        try:
            # Add to active requests
            self.active_requests[request_id] = {
                "start_time": start_time,
                "status": "processing"
            }
            
            # Get model for inference
            model, version = self.version_manager.get_model_for_inference(request_id)
            
            # Process input
            if not TORCH_AVAILABLE:
                raise RuntimeError("PyTorch not available for inference")
            
            # Convert input to tensor
            input_data = self._prepare_input(request["input"])
            
            # Run inference
            with torch.no_grad():
                if torch.cuda.is_available():
                    input_data = input_data.cuda()
                    model = model.cuda()
                
                output = model(input_data)
                
                if isinstance(output, tuple):
                    output = output[0]  # Take first output if tuple
                
                # Convert back to CPU for JSON serialization
                if hasattr(output, 'cpu'):
                    output = output.cpu().numpy().tolist()
            
            # Calculate metrics
            end_time = time.time()
            latency = end_time - start_time
            
            # Record metrics
            self.version_manager.record_inference_metrics(version, latency, True)
            self.total_requests += 1
            self.total_latency += latency
            
            # Clean up active requests
            if request_id in self.active_requests:
                del self.active_requests[request_id]
            
            return {
                "request_id": request_id,
                "output": output,
                "model_version": version,
                "latency_ms": latency * 1000,
                "status": "success"
            }
            
        except Exception as e:
            # Handle errors
            end_time = time.time()
            latency = end_time - start_time
            
            # Record error metrics
            if 'version' in locals():
                self.version_manager.record_inference_metrics(version, latency, False)
            
            self.total_errors += 1
            
            # Clean up
            if request_id in self.active_requests:
                del self.active_requests[request_id]
            
            logger.error(f"Inference error for request {request_id}: {str(e)}")
            
            return {
                "request_id": request_id,
                "error": str(e),
                "latency_ms": latency * 1000,
                "status": "error"
            }
    
    def _prepare_input(self, input_data: Any) -> torch.Tensor:
        """Prepare input data for inference."""
        if isinstance(input_data, list):
            # Convert list to tensor
            try:
                tensor = torch.tensor(input_data, dtype=torch.float32)
                
                # Ensure 4D shape for QISA: [batch, seq, spatial, features]
                if tensor.dim() == 2:
                    # [seq, features] -> [1, seq, 1, features]
                    tensor = tensor.unsqueeze(0).unsqueeze(2)
                elif tensor.dim() == 3:
                    # [batch, seq, features] -> [batch, seq, 1, features] 
                    tensor = tensor.unsqueeze(2)
                elif tensor.dim() == 4:
                    # Already correct shape
                    pass
                else:
                    raise ValueError(f"Unsupported input dimensions: {tensor.dim()}")
                
                return tensor
                
            except Exception as e:
                raise ValueError(f"Failed to convert input to tensor: {e}")
        
        else:
            raise ValueError(f"Unsupported input type: {type(input_data)}")
    
    async def batch_processing_loop(self):
        """Process batched requests for efficiency."""
        while True:
            try:
                # This would implement batch processing logic
                await asyncio.sleep(0.1)  # Placeholder
            except Exception as e:
                logger.error(f"Batch processing error: {e}")
                await asyncio.sleep(1.0)
    
    async def health_monitoring_loop(self):
        """Monitor server health."""
        while True:
            try:
                await self.health_monitor.check_health()
                await asyncio.sleep(self.config.health_check_interval)
            except Exception as e:
                logger.error(f"Health monitoring error: {e}")
                await asyncio.sleep(10.0)
    
    async def metrics_collection_loop(self):
        """Collect and report metrics."""
        while True:
            try:
                metrics = self.get_server_metrics()
                # In production, would send to monitoring system
                logger.debug(f"Server metrics: {metrics}")
                await asyncio.sleep(60.0)  # Report every minute
            except Exception as e:
                logger.error(f"Metrics collection error: {e}")
                await asyncio.sleep(60.0)
    
    def get_server_metrics(self) -> Dict[str, Any]:
        """Get comprehensive server metrics."""
        total_requests = self.total_requests
        
        return {
            "server": {
                "total_requests": total_requests,
                "total_errors": self.total_errors,
                "error_rate_percent": (self.total_errors / total_requests * 100) if total_requests > 0 else 0,
                "average_latency_ms": (self.total_latency / total_requests * 1000) if total_requests > 0 else 0,
                "active_requests": len(self.active_requests),
                "uptime_seconds": time.time() - getattr(self, '_start_time', time.time())
            },
            "models": self.version_manager.get_model_metrics(),
            "health": self.health_monitor.get_health_status(),
            "timestamp": datetime.now().isoformat()
        }


class BatchProcessor:
    """Processes requests in batches for efficiency."""
    
    def __init__(self, config: QISADeploymentConfig):
        self.config = config
        self.pending_requests = []
        self.batch_lock = threading.Lock()
    
    def add_request(self, request: Dict[str, Any]) -> str:
        """Add request to batch queue."""
        request_id = str(uuid.uuid4())
        
        with self.batch_lock:
            self.pending_requests.append({
                "id": request_id,
                "data": request,
                "timestamp": time.time()
            })
        
        return request_id
    
    def create_batch(self) -> List[Dict[str, Any]]:
        """Create batch from pending requests."""
        with self.batch_lock:
            if not self.pending_requests:
                return []
            
            batch_size = min(len(self.pending_requests), self.config.max_batch_size)
            batch = self.pending_requests[:batch_size]
            self.pending_requests = self.pending_requests[batch_size:]
            
            return batch


class HealthMonitor:
    """Monitors system and model health."""
    
    def __init__(self, config: QISADeploymentConfig):
        self.config = config
        self.health_history = []
        
    async def check_health(self) -> Dict[str, Any]:
        """Perform comprehensive health check."""
        health_status = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "checks": {},
            "issues": []
        }
        
        try:
            # System resource check
            if PSUTIL_AVAILABLE:
                cpu_percent = psutil.cpu_percent(interval=1)
                memory = psutil.virtual_memory()
                
                health_status["checks"]["cpu"] = {
                    "usage_percent": cpu_percent,
                    "status": "healthy" if cpu_percent < 90 else "warning"
                }
                
                health_status["checks"]["memory"] = {
                    "usage_percent": memory.percent,
                    "available_gb": memory.available / (1024**3),
                    "status": "healthy" if memory.percent < 90 else "warning"
                }
                
                if cpu_percent > 95:
                    health_status["issues"].append("High CPU usage")
                if memory.percent > 95:
                    health_status["issues"].append("High memory usage")
            
            # GPU check
            if TORCH_AVAILABLE and torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100
                health_status["checks"]["gpu"] = {
                    "memory_usage_percent": gpu_memory,
                    "status": "healthy" if gpu_memory < 90 else "warning"
                }
                
                if gpu_memory > 95:
                    health_status["issues"].append("High GPU memory usage")
            
            # Model availability check
            # This would check if models are loaded and responsive
            
            # Set overall status
            if health_status["issues"]:
                health_status["status"] = "warning" if len(health_status["issues"]) < 3 else "unhealthy"
        
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
        
        self.health_history.append(health_status)
        
        # Keep only recent history
        if len(self.health_history) > 100:
            self.health_history = self.health_history[-100:]
        
        return health_status
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get current health status."""
        if self.health_history:
            return self.health_history[-1]
        else:
            return {"status": "unknown", "message": "No health checks performed yet"}


class RateLimiter:
    """Simple rate limiter for API requests."""
    
    def __init__(self, requests_per_minute: int):
        self.requests_per_minute = requests_per_minute
        self.requests = []
    
    def allow_request(self) -> bool:
        """Check if request is allowed based on rate limit."""
        now = time.time()
        
        # Remove old requests (older than 1 minute)
        self.requests = [req_time for req_time in self.requests if now - req_time < 60]
        
        # Check if under limit
        if len(self.requests) < self.requests_per_minute:
            self.requests.append(now)
            return True
        
        return False


class AutoScaler:
    """Automatic scaling based on load and performance."""
    
    def __init__(self, config: QISADeploymentConfig):
        self.config = config
        self.current_replicas = config.min_replicas
        self.scaling_history = []
        
    def should_scale_up(self, metrics: Dict[str, Any]) -> bool:
        """Determine if scaling up is needed."""
        if self.current_replicas >= self.config.max_replicas:
            return False
        
        # Check CPU and memory utilization
        cpu_usage = metrics.get("cpu", {}).get("usage_percent", 0)
        memory_usage = metrics.get("memory", {}).get("usage_percent", 0)
        
        return (cpu_usage > self.config.scale_up_threshold or 
                memory_usage > self.config.scale_up_threshold)
    
    def should_scale_down(self, metrics: Dict[str, Any]) -> bool:
        """Determine if scaling down is needed."""
        if self.current_replicas <= self.config.min_replicas:
            return False
        
        cpu_usage = metrics.get("cpu", {}).get("usage_percent", 0)
        memory_usage = metrics.get("memory", {}).get("usage_percent", 0)
        
        return (cpu_usage < self.config.scale_down_threshold and 
                memory_usage < self.config.scale_down_threshold)
    
    def scale_up(self) -> bool:
        """Scale up replicas."""
        if self.current_replicas < self.config.max_replicas:
            old_replicas = self.current_replicas
            self.current_replicas += 1
            
            self.scaling_history.append({
                "timestamp": datetime.now(),
                "action": "scale_up",
                "from": old_replicas,
                "to": self.current_replicas
            })
            
            logger.info(f"Scaled up from {old_replicas} to {self.current_replicas} replicas")
            return True
        
        return False
    
    def scale_down(self) -> bool:
        """Scale down replicas."""
        if self.current_replicas > self.config.min_replicas:
            old_replicas = self.current_replicas
            self.current_replicas -= 1
            
            self.scaling_history.append({
                "timestamp": datetime.now(),
                "action": "scale_down", 
                "from": old_replicas,
                "to": self.current_replicas
            })
            
            logger.info(f"Scaled down from {old_replicas} to {self.current_replicas} replicas")
            return True
        
        return False


def create_production_server(model: nn.Module, config: QISADeploymentConfig = None) -> QISAInferenceServer:
    """Create production-ready QISA inference server."""
    if config is None:
        config = QISADeploymentConfig()
    
    server = QISAInferenceServer(config)
    
    # Register the model
    server.version_manager.register_model(
        version=config.model_version,
        model=model,
        metadata={
            "created_at": datetime.now().isoformat(),
            "model_type": "QISA",
            "parameters": sum(p.numel() for p in model.parameters()) if TORCH_AVAILABLE else 0
        }
    )
    
    return server


async def run_production_server(server: QISAInferenceServer):
    """Run the production server with HTTP API."""
    if not AIOHTTP_AVAILABLE:
        logger.error("aiohttp not available - cannot run HTTP server")
        return
    
    app = web.Application()
    
    # Health check endpoint
    async def health_check(request):
        health_status = server.health_monitor.get_health_status()
        status = 200 if health_status["status"] == "healthy" else 503
        return web.json_response(health_status, status=status)
    
    # Inference endpoint
    async def inference(request):
        try:
            data = await request.json()
            api_key = request.headers.get("X-API-Key")
            
            result = await server.handle_inference_request(data, api_key)
            status = 200 if result["status"] == "success" else 400
            
            return web.json_response(result, status=status)
            
        except Exception as e:
            return web.json_response(
                {"error": str(e), "status": "error"},
                status=500
            )
    
    # Metrics endpoint
    async def metrics(request):
        metrics = server.get_server_metrics()
        return web.json_response(metrics)
    
    # Model management endpoints
    async def list_models(request):
        models = server.version_manager.models
        model_info = {}
        
        for version, info in models.items():
            model_info[version] = {
                "metadata": info["metadata"],
                "registration_time": info["registration_time"].isoformat(),
                "inference_count": info["inference_count"]
            }
        
        return web.json_response({
            "models": model_info,
            "active_version": server.version_manager.active_version,
            "canary_version": server.version_manager.canary_version
        })
    
    # Setup routes
    app.router.add_get("/health", health_check)
    app.router.add_post("/inference", inference)
    app.router.add_get("/metrics", metrics)
    app.router.add_get("/models", list_models)
    
    # Initialize server
    await server.initialize()
    
    # Add sample API key
    server.add_api_key("qisa-demo-key-12345")
    
    # Start server
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', server.config.serving_port)
    
    logger.info(f"Starting QISA production server on port {server.config.serving_port}")
    await site.start()
    
    # Keep running
    try:
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        logger.info("Shutting down server...")
        await runner.cleanup()


# Example usage and testing
if __name__ == "__main__":
    print("üöÄ QISA Production Deployment Demo")
    
    if not TORCH_AVAILABLE:
        print("‚ùå PyTorch not available - cannot run deployment demo")
        print("   This module provides production deployment capabilities for QISA models")
    elif not AIOHTTP_AVAILABLE:
        print("‚ùå aiohttp not available - cannot run HTTP server")
        print("   Install with: pip install aiohttp")
        print("   The deployment classes are available for use")
    else:
        print("‚úÖ All dependencies available - ready for production deployment")
        print("   Use create_production_server() to deploy QISA models")