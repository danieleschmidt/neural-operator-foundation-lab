name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks weekly to track performance trends
    - cron: '0 4 * * 0'  # Sunday at 4 AM UTC

env:
  PYTHONUNBUFFERED: 1

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need history for performance comparison
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        pip install pytest-benchmark memory-profiler py-spy
        
    - name: Run performance tests
      env:
        CUDA_VISIBLE_DEVICES: ""  # CPU-only for consistent benchmarks
      run: |
        pytest tests/performance/ \
          -m performance \
          --benchmark-json=benchmark.json \
          --benchmark-min-rounds=5 \
          --benchmark-warmup=on \
          --benchmark-disable-gc \
          --tb=short
          
    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'pytest'
        output-file-path: benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: ${{ github.ref == 'refs/heads/main' }}
        comment-on-alert: true
        alert-threshold: '150%'
        alert-comment-cc-users: '@maintainer1,@maintainer2'
        fail-on-alert: false
        
    - name: Memory profiling
      run: |
        python -m memory_profiler tests/performance/test_memory_usage.py > memory_profile.txt
        
    - name: Upload performance reports
      uses: actions/upload-artifact@v3
      with:
        name: performance-reports
        path: |
          benchmark.json
          memory_profile.txt

  gpu-benchmark:
    name: GPU Performance Benchmarks
    runs-on: [self-hosted, gpu]  # Requires self-hosted GPU runner
    if: github.repository_owner == 'terragon-labs'  # Only run on main repo
    timeout-minutes: 45
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
        
    - name: Install CUDA dependencies
      run: |
        python -m pip install --upgrade pip
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
        pip install -e ".[dev,test]"
        
    - name: Check GPU availability
      run: |
        nvidia-smi
        python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
        python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"
        
    - name: Run GPU benchmarks
      run: |
        pytest tests/performance/ \
          -m "performance and gpu" \
          --benchmark-json=gpu_benchmark.json \
          --benchmark-min-rounds=3 \
          --tb=short
          
    - name: Upload GPU benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: gpu-performance-reports
        path: gpu_benchmark.json

  regression-test:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    needs: [benchmark]
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout PR branch
      uses: actions/checkout@v4
      with:
        ref: ${{ github.head_ref }}
        
    - name: Checkout base branch
      uses: actions/checkout@v4
      with:
        ref: ${{ github.base_ref }}
        path: baseline
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        
    - name: Run baseline benchmarks
      run: |
        cd baseline
        pip install -e ".[dev,test]"
        pytest tests/performance/ \
          -m performance \
          --benchmark-json=baseline_benchmark.json \
          --benchmark-min-rounds=3 \
          --tb=short
          
    - name: Run current benchmarks
      run: |
        pytest tests/performance/ \
          -m performance \
          --benchmark-json=current_benchmark.json \
          --benchmark-min-rounds=3 \
          --tb=short
          
    - name: Compare performance
      run: |
        python scripts/compare_benchmarks.py \
          baseline/baseline_benchmark.json \
          current_benchmark.json \
          > performance_comparison.md
          
    - name: Comment PR with results
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const comparison = fs.readFileSync('performance_comparison.md', 'utf8');
          
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## Performance Comparison\n\n${comparison}`
          });

  scalability-test:
    name: Scalability Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || contains(github.event.pull_request.labels.*.name, 'performance')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        
    - name: Run scalability tests
      run: |
        pytest tests/performance/test_scalability.py \
          --scalability-test \
          --json-report=scalability_report.json \
          --tb=short
          
    - name: Generate scalability report
      run: |
        python scripts/generate_scalability_report.py \
          scalability_report.json \
          > scalability_summary.md
          
    - name: Upload scalability results
      uses: actions/upload-artifact@v3
      with:
        name: scalability-reports
        path: |
          scalability_report.json
          scalability_summary.md

  memory-leak-test:
    name: Memory Leak Detection
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || contains(github.event.pull_request.labels.*.name, 'memory')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        pip install pympler objgraph
        
    - name: Run memory leak tests
      run: |
        pytest tests/performance/test_memory_leaks.py \
          --memory-leak-test \
          --json-report=memory_leak_report.json \
          --tb=short
          
    - name: Upload memory leak results
      uses: actions/upload-artifact@v3
      with:
        name: memory-leak-reports
        path: memory_leak_report.json

  performance-report:
    name: Generate Performance Report
    runs-on: ubuntu-latest
    needs: [benchmark, regression-test, scalability-test, memory-leak-test]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      
    - name: Generate comprehensive report
      run: |
        python scripts/generate_performance_report.py \
          --benchmark-file performance-reports/benchmark.json \
          --scalability-file scalability-reports/scalability_report.json \
          --memory-file memory-leak-reports/memory_leak_report.json \
          --output performance_report.html
          
    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: performance_report.html
        
    - name: Deploy report to GitHub Pages
      if: github.ref == 'refs/heads/main'
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./
        destination_dir: performance
        keep_files: true