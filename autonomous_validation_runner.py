#!/usr/bin/env python3
"""Autonomous validation runner for TERRAGON SDLC v4.0."""

import sys
import os
import logging
import time
import json
from pathlib import Path
from typing import Dict, List, Any, Optional

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('autonomous_validation')


class AutonomousValidator:
    """Autonomous validation system for the neural operator framework."""
    
    def __init__(self):
        self.validation_results = {}
        self.start_time = time.time()
        
    def validate_project_structure(self) -> Dict[str, Any]:
        """Validate project structure and organization."""
        logger.info("Validating project structure...")
        
        required_dirs = [
            'src/neural_operator_lab',
            'src/neural_operator_lab/models',
            'src/neural_operator_lab/autonomous',
            'src/neural_operator_lab/resilience',
            'src/neural_operator_lab/security',
            'src/neural_operator_lab/performance',
            'src/neural_operator_lab/testing',
            'tests',
            'deployment',
            'docs'
        ]
        
        structure_validation = {
            'directories_present': 0,
            'directories_missing': [],
            'total_required': len(required_dirs)
        }
        
        for required_dir in required_dirs:
            if Path(required_dir).exists():
                structure_validation['directories_present'] += 1
            else:
                structure_validation['directories_missing'].append(required_dir)
        
        structure_validation['success'] = structure_validation['directories_present'] == structure_validation['total_required']
        
        return structure_validation
    
    def validate_implementation_completeness(self) -> Dict[str, Any]:
        """Validate implementation completeness across all generations."""
        logger.info("Validating implementation completeness...")
        
        # Generation 1: Simple implementation
        gen1_components = [
            'src/neural_operator_lab/models/quantum_neural_operator.py',
            'src/neural_operator_lab/autonomous/self_improving_trainer.py'
        ]
        
        # Generation 2: Robust implementation  
        gen2_components = [
            'src/neural_operator_lab/resilience/fault_tolerant_training.py',
            'src/neural_operator_lab/security/advanced_security_framework.py'
        ]
        
        # Generation 3: Optimized implementation
        gen3_components = [
            'src/neural_operator_lab/performance/quantum_accelerated_optimization.py',
            'src/neural_operator_lab/testing/autonomous_test_generation.py'
        ]
        
        completeness_validation = {
            'generation_1': self._check_components(gen1_components),
            'generation_2': self._check_components(gen2_components),
            'generation_3': self._check_components(gen3_components)
        }
        
        total_components = len(gen1_components) + len(gen2_components) + len(gen3_components)
        present_components = (
            completeness_validation['generation_1']['present'] +
            completeness_validation['generation_2']['present'] +
            completeness_validation['generation_3']['present']
        )
        
        completeness_validation['overall_completion'] = present_components / total_components
        completeness_validation['success'] = completeness_validation['overall_completion'] >= 0.95
        
        return completeness_validation
    
    def _check_components(self, components: List[str]) -> Dict[str, int]:
        """Check if components exist and get basic metrics."""
        present = 0
        total_lines = 0
        
        for component in components:
            if Path(component).exists():
                present += 1
                try:
                    with open(component, 'r') as f:
                        total_lines += len(f.readlines())
                except Exception:
                    pass
        
        return {
            'present': present,
            'total': len(components),
            'total_lines': total_lines
        }
    
    def validate_quality_gates(self) -> Dict[str, Any]:
        """Validate quality gates implementation."""
        logger.info("Validating quality gates...")
        
        quality_components = {
            'progressive_gates': 'src/neural_operator_lab/quality_gates/progressive_gates.py',
            'research_gates': 'src/neural_operator_lab/quality_gates/research_quality_gates.py',
            'enhanced_security': 'src/neural_operator_lab/quality_gates/enhanced_security_gates.py',
            'autonomous_validation': 'src/neural_operator_lab/quality_gates/autonomous_validation.py'
        }
        
        quality_validation = {}
        
        for gate_name, gate_path in quality_components.items():
            if Path(gate_path).exists():
                try:
                    with open(gate_path, 'r') as f:
                        content = f.read()
                        quality_validation[gate_name] = {
                            'exists': True,
                            'lines': len(content.split('\n')),
                            'has_classes': 'class ' in content,
                            'has_functions': 'def ' in content,
                            'complexity': 'high' if len(content) > 5000 else 'medium' if len(content) > 1000 else 'low'
                        }
                except Exception as e:
                    quality_validation[gate_name] = {'exists': True, 'error': str(e)}
            else:
                quality_validation[gate_name] = {'exists': False}
        
        # Overall quality gate success
        existing_gates = sum(1 for gate in quality_validation.values() if gate.get('exists', False))
        quality_validation['success'] = existing_gates >= len(quality_components) * 0.8
        quality_validation['completion_rate'] = existing_gates / len(quality_components)
        
        return quality_validation
    
    def validate_security_implementation(self) -> Dict[str, Any]:
        """Validate security implementation."""
        logger.info("Validating security implementation...")
        
        security_features = [
            'encryption', 'integrity_checks', 'access_control',
            'audit_logging', 'input_validation', 'differential_privacy'
        ]
        
        security_validation = {
            'features_implemented': [],
            'features_missing': [],
            'security_score': 0.0
        }
        
        # Check advanced security framework
        security_file = 'src/neural_operator_lab/security/advanced_security_framework.py'
        if Path(security_file).exists():
            try:
                with open(security_file, 'r') as f:
                    content = f.read()
                    
                    for feature in security_features:
                        # Simple keyword matching for validation
                        if feature.lower() in content.lower():
                            security_validation['features_implemented'].append(feature)
                        else:
                            security_validation['features_missing'].append(feature)
                
                security_validation['security_score'] = (
                    len(security_validation['features_implemented']) / len(security_features)
                )
                
            except Exception as e:
                security_validation['error'] = str(e)
        else:
            security_validation['features_missing'] = security_features
            security_validation['error'] = 'Security framework file not found'
        
        security_validation['success'] = security_validation['security_score'] >= 0.8
        
        return security_validation
    
    def validate_autonomous_capabilities(self) -> Dict[str, Any]:
        """Validate autonomous capabilities implementation."""
        logger.info("Validating autonomous capabilities...")
        
        autonomous_features = [
            'self_improving_trainer', 'evolutionary_optimization',
            'neural_architecture_search', 'auto_scaling',
            'fault_tolerance', 'quantum_optimization'
        ]
        
        autonomous_validation = {
            'capabilities_implemented': [],
            'capabilities_missing': [],
            'autonomy_score': 0.0
        }
        
        # Check multiple files for autonomous features
        autonomous_files = [
            'src/neural_operator_lab/autonomous/self_improving_trainer.py',
            'src/neural_operator_lab/performance/quantum_accelerated_optimization.py',
            'src/neural_operator_lab/resilience/fault_tolerant_training.py'
        ]
        
        all_content = ""
        for file_path in autonomous_files:
            if Path(file_path).exists():
                try:
                    with open(file_path, 'r') as f:
                        all_content += f.read().lower()
                except Exception:
                    pass
        
        for capability in autonomous_features:
            # Check for capability keywords
            capability_keywords = capability.replace('_', ' ').split()
            if any(keyword in all_content for keyword in capability_keywords):
                autonomous_validation['capabilities_implemented'].append(capability)
            else:
                autonomous_validation['capabilities_missing'].append(capability)
        
        autonomous_validation['autonomy_score'] = (
            len(autonomous_validation['capabilities_implemented']) / len(autonomous_features)
        )
        
        autonomous_validation['success'] = autonomous_validation['autonomy_score'] >= 0.8
        
        return autonomous_validation
    
    def validate_research_readiness(self) -> Dict[str, Any]:
        """Validate research publication readiness."""
        logger.info("Validating research readiness...")
        
        research_components = [
            'novel_algorithms', 'benchmark_suite', 'statistical_analysis',
            'reproducibility', 'documentation', 'experimental_framework'
        ]
        
        research_validation = {
            'components_ready': [],
            'components_missing': [],
            'research_score': 0.0
        }
        
        # Check research files
        research_files = [
            'src/neural_operator_lab/research/experiment_framework.py',
            'src/neural_operator_lab/research/statistical_analysis.py',
            'src/neural_operator_lab/research/benchmark_suite.py',
            'src/neural_operator_lab/models/quantum_neural_operator.py'
        ]
        
        research_content = ""
        for file_path in research_files:
            if Path(file_path).exists():
                try:
                    with open(file_path, 'r') as f:
                        research_content += f.read().lower()
                except Exception:
                    pass
        
        # Check for research components
        component_keywords = {
            'novel_algorithms': ['quantum', 'superposition', 'entanglement', 'annealing'],
            'benchmark_suite': ['benchmark', 'evaluation', 'metrics'],
            'statistical_analysis': ['statistical', 'significance', 'p_value'],
            'reproducibility': ['reproducible', 'seed', 'deterministic'],
            'documentation': ['docstring', '\"\"\"', 'documentation'],
            'experimental_framework': ['experiment', 'trial', 'framework']
        }
        
        for component, keywords in component_keywords.items():
            if any(keyword in research_content for keyword in keywords):
                research_validation['components_ready'].append(component)
            else:
                research_validation['components_missing'].append(component)
        
        research_validation['research_score'] = (
            len(research_validation['components_ready']) / len(research_components)
        )
        
        research_validation['success'] = research_validation['research_score'] >= 0.7
        
        return research_validation
    
    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """Run comprehensive autonomous validation."""
        logger.info("Starting comprehensive autonomous validation...")
        
        validation_suite = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'validator': 'TERRAGON SDLC v4.0 Autonomous Validator',
            'project_structure': self.validate_project_structure(),
            'implementation_completeness': self.validate_implementation_completeness(),
            'quality_gates': self.validate_quality_gates(),
            'security_implementation': self.validate_security_implementation(),
            'autonomous_capabilities': self.validate_autonomous_capabilities(),
            'research_readiness': self.validate_research_readiness()
        }
        
        # Calculate overall success metrics
        success_checks = [
            validation_suite['project_structure']['success'],
            validation_suite['implementation_completeness']['success'],
            validation_suite['quality_gates']['success'],
            validation_suite['security_implementation']['success'],
            validation_suite['autonomous_capabilities']['success'],
            validation_suite['research_readiness']['success']
        ]
        
        validation_suite['overall_success'] = sum(success_checks) / len(success_checks)
        validation_suite['validation_duration'] = time.time() - self.start_time
        validation_suite['ready_for_production'] = validation_suite['overall_success'] >= 0.85
        
        return validation_suite
    
    def generate_validation_report(self, validation_results: Dict[str, Any]) -> str:
        """Generate comprehensive validation report."""
        report_lines = [
            "# TERRAGON SDLC v4.0 - Autonomous Validation Report",
            f"Generated: {validation_results['timestamp']}",
            f"Validation Duration: {validation_results['validation_duration']:.2f}s",
            "",
            f"## Overall Success: {validation_results['overall_success']:.1%}",
            f"**Production Ready: {'‚úÖ YES' if validation_results['ready_for_production'] else '‚ùå NO'}**",
            "",
            "## Validation Results Summary",
            ""
        ]
        
        # Project Structure
        struct_result = validation_results['project_structure']
        report_lines.append(f"### üìÅ Project Structure: {'‚úÖ' if struct_result['success'] else '‚ùå'}")
        report_lines.append(f"- Directories Present: {struct_result['directories_present']}/{struct_result['total_required']}")
        if struct_result['directories_missing']:
            report_lines.append(f"- Missing: {', '.join(struct_result['directories_missing'])}")
        report_lines.append("")
        
        # Implementation Completeness
        impl_result = validation_results['implementation_completeness']
        report_lines.append(f"### üöÄ Implementation Completeness: {'‚úÖ' if impl_result['success'] else '‚ùå'}")
        report_lines.append(f"- Overall Completion: {impl_result['overall_completion']:.1%}")
        report_lines.append(f"- Generation 1: {impl_result['generation_1']['present']}/{impl_result['generation_1']['total']} components")
        report_lines.append(f"- Generation 2: {impl_result['generation_2']['present']}/{impl_result['generation_2']['total']} components")
        report_lines.append(f"- Generation 3: {impl_result['generation_3']['present']}/{impl_result['generation_3']['total']} components")
        report_lines.append("")
        
        # Quality Gates
        quality_result = validation_results['quality_gates']
        report_lines.append(f"### üõ°Ô∏è Quality Gates: {'‚úÖ' if quality_result['success'] else '‚ùå'}")
        report_lines.append(f"- Completion Rate: {quality_result['completion_rate']:.1%}")
        for gate_name, gate_info in quality_result.items():
            if isinstance(gate_info, dict) and 'exists' in gate_info:
                status = '‚úÖ' if gate_info['exists'] else '‚ùå'
                report_lines.append(f"- {gate_name}: {status}")
        report_lines.append("")
        
        # Security
        security_result = validation_results['security_implementation']
        report_lines.append(f"### üîí Security Implementation: {'‚úÖ' if security_result['success'] else '‚ùå'}")
        report_lines.append(f"- Security Score: {security_result['security_score']:.1%}")
        report_lines.append(f"- Features Implemented: {len(security_result['features_implemented'])}")
        report_lines.append(f"- Features Missing: {len(security_result['features_missing'])}")
        report_lines.append("")
        
        # Autonomous Capabilities
        auto_result = validation_results['autonomous_capabilities']
        report_lines.append(f"### ü§ñ Autonomous Capabilities: {'‚úÖ' if auto_result['success'] else '‚ùå'}")
        report_lines.append(f"- Autonomy Score: {auto_result['autonomy_score']:.1%}")
        report_lines.append(f"- Capabilities Implemented: {len(auto_result['capabilities_implemented'])}")
        report_lines.append("")
        
        # Research Readiness
        research_result = validation_results['research_readiness']
        report_lines.append(f"### üî¨ Research Readiness: {'‚úÖ' if research_result['success'] else '‚ùå'}")
        report_lines.append(f"- Research Score: {research_result['research_score']:.1%}")
        report_lines.append(f"- Components Ready: {len(research_result['components_ready'])}")
        report_lines.append("")
        
        # Recommendations
        report_lines.extend([
            "## Recommendations",
            ""
        ])
        
        if validation_results['overall_success'] >= 0.95:
            report_lines.append("üéâ **Excellent!** Implementation exceeds target quality metrics.")
        elif validation_results['overall_success'] >= 0.85:
            report_lines.append("‚úÖ **Good!** Implementation meets production readiness criteria.")
        elif validation_results['overall_success'] >= 0.70:
            report_lines.append("‚ö†Ô∏è **Needs Improvement** - Address failing quality gates before production.")
        else:
            report_lines.append("‚ùå **Major Issues** - Significant improvements required.")
        
        return '\n'.join(report_lines)


def main():
    """Main validation runner."""
    print("üöÄ Starting TERRAGON SDLC v4.0 Autonomous Validation...")
    print("=" * 60)
    
    # Create validator
    validator = AutonomousValidator()
    
    try:
        # Run comprehensive validation
        results = validator.run_comprehensive_validation()
        
        # Generate report
        report = validator.generate_validation_report(results)
        
        # Save results
        results_file = Path('autonomous_validation_results.json')
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        # Save report
        report_file = Path('AUTONOMOUS_VALIDATION_REPORT.md')
        with open(report_file, 'w') as f:
            f.write(report)
        
        # Print summary
        print("\n" + "=" * 60)
        print("üéØ AUTONOMOUS VALIDATION COMPLETE")
        print("=" * 60)
        print(f"Overall Success: {results['overall_success']:.1%}")
        print(f"Production Ready: {'‚úÖ YES' if results['ready_for_production'] else '‚ùå NO'}")
        print(f"Duration: {results['validation_duration']:.2f}s")
        print(f"\nReports saved:")
        print(f"- JSON Results: {results_file}")
        print(f"- Markdown Report: {report_file}")
        
        # Print key metrics
        print("\nüìä Key Metrics:")
        print(f"- Implementation: {results['implementation_completeness']['overall_completion']:.1%}")
        print(f"- Security: {results['security_implementation']['security_score']:.1%}")
        print(f"- Autonomy: {results['autonomous_capabilities']['autonomy_score']:.1%}")
        print(f"- Research: {results['research_readiness']['research_score']:.1%}")
        
        return 0 if results['ready_for_production'] else 1
        
    except Exception as e:
        logger.error(f"Validation failed: {e}")
        print(f"‚ùå VALIDATION FAILED: {e}")
        return 1


if __name__ == "__main__":
    exit(main())